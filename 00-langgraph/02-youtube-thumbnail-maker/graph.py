from langgraph.graph import END, START, StateGraph
from langgraph.types import Send, interrupt, Command
from typing import TypedDict
import subprocess
from openai import OpenAI
import textwrap
from langchain.chat_models import init_chat_model
from typing_extensions import Annotated
import operator
import base64


llm = init_chat_model("openai:gpt-4o-mini")


class State(TypedDict):

    video_file: str  # ë¹„ë””ì˜¤íŒŒì¼ ê²½ë¡œ
    audio_file: str  # ì˜¤ë””ì˜¤íŒŒì¼ ê²½ë¡œ
    transcription: str  # ì¶”ì¶œëœ ìŒì„±
    summaries: Annotated[list[str], operator.add]  # ìš”ì•½ ë¬¸ì¥ ëª©ë¡
    final_summary: str

    thumbnail_prompts: Annotated[list[str], operator.add]
    thumbnail_sketches: Annotated[list[str], operator.add]

    user_feedback: str
    chosen_prompt: str


def extract_audio(state: State):
    # Using ffmpeg to extract audio from video
    output_file = state["video_file"].replace("mp4", "mp3")
    command = [
        "ffmpeg",
        "-i",
        state["video_file"],
        "-filter:a",
        "atempo=1.7",
        "-y",
        output_file
    ]
    subprocess.run(command)
    return {
        "audio_file": output_file
    }


WHISPER_PROMPT = (
    "ì´ ì˜¤ë””ì˜¤ëŠ” â€˜ëˆíƒ€í´ë¡œìŠ¤â€™ ìœ íŠœë¸Œ ì±„ë„ì˜ â€˜ìƒí‘œë“±ë¡ ë°©ë²•â€™ ì•ˆë‚´ì…ë‹ˆë‹¤. í•œêµ­ íŠ¹í—ˆì²­(KIPO)ê³¼ KIPRISë¥¼ ì „ì œë¡œ ìƒí‘œê²€ìƒ‰, ì¶œì›, ì‹¬ì‚¬, ë³´ì •, "
    "ì˜ê²¬ì œì¶œí†µì§€, ê±°ì ˆì´ìœ , ë“±ë¡ê²°ì •, ê°±ì‹ ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ë‹ˆìŠ¤ êµ­ì œë¶„ë¥˜(1~45ë¥˜), ìœ ì‚¬êµ°ì½”ë“œ, ì§€ì •ìƒí’ˆ, "
    "ì¶œì›ìˆ˜ìˆ˜ë£Œ/ë“±ë¡ë£Œ ë“± ìˆ«ìì™€ ë¶„ë¥˜í‘œê¸°ëŠ” ì •í™•íˆ ìˆ«ìë¡œ í‘œê¸°í•˜ì„¸ìš”(ì˜ˆ: 35ë¥˜, 45ë¥˜). "
    "ë¸Œëœë“œÂ·ì…€ëŸ¬(ì¿ íŒ¡, ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´, ì•„ë§ˆì¡´)ëŠ” í•œê¸€ ê·¸ëŒ€ë¡œ í‘œê¸°í•˜ê³ , ê³ ìœ ëª…ì‚¬ëŠ” ì˜¤íƒˆì ì—†ì´ ì ìŠµë‹ˆë‹¤. "
    "í•µì‹¬ ìš©ì–´: ìƒí‘œë“±ë¡, ìƒí‘œì¶œì›, ìƒí‘œê²€ìƒ‰, ì§€ì‹ì¬ì‚°ê¶Œ, ë¸Œëœë“œë³´í˜¸, ì§€ì •ìƒí’ˆ, ìœ ì‚¬êµ°ì½”ë“œ, ì¶œì›ë²ˆí˜¸, ê±°ì ˆì´ìœ , "
    "ë“±ë¡ê²°ì •, ê°±ì‹ , KIPRIS, íŠ¹í—ˆì²­. "
    "English terms: KIPO, KIPRIS, Nice Classification, Classes 1â€“45, trademark search, office action, refusal, registration decision, renewal."
)


def transcribe_audio(state: State):
    # use audio file
    client = OpenAI()
    with open(state["audio_file"], "rb") as audio_file:
        transcription = client.audio.transcriptions.create(
            model="whisper-1",
            response_format="text",
            file=audio_file,
            language="ko",
            prompt=WHISPER_PROMPT
        )
        return {
            "transcription": transcription
        }


def dispatch_summarizers(state: State):
    transcription = state["transcription"]
    chunks = []
    for idx, chunk in enumerate(textwrap.wrap(transcription, 500)):
        chunks.append({
            "id": idx+1,
            "chunk": chunk
        })
    return [Send("summarize_chunk", chunk) for chunk in chunks]


def summarize_chunk(chunk):
    chunk_id = chunk["id"]
    chunk_text = chunk["chunk"]

    response = llm.invoke(
        f"""ì´ ë¬¸ì¥ì„ ìš”ì•½í•´ì¤˜.
            Text : {chunk_text}"""
    )
    summary = f"[Chunk {chunk_id}] {response.content}"
    return {
        "summaries": [summary]
    }


def finalize_summary(state: State):
    all_summaries = "\n".join(state["summaries"])

    prompt = f"""
        you are given multiple summarise of different chunks from a video transcription.
        
        Please create a comprehensive final summary the combines all the key points.

        Individual summaries :        

        {all_summaries}
    """

    response = llm.invoke(prompt)
    return {
        "final_summary": response.content
    }


def dispatch_artists(state: State):
    # return [Send("generate_thumbnail", state) for i in [1, 2, 3]]
    return [
        Send(
            "generate_thumbnail",
            {
                "id": i,
                "summary": state["final_summary"],
            }
        )
        for i in [1, 2, 3, 4, 5]
    ]


def generate_thumbnail(args):
    concept_id = args["id"]
    summary = args["summary"]

    prompt = f"""
        You are an expert Korean thumbnail designer AI generating an image with the GPT-image-1 model.

        ğŸ¯ GOAL:
        Create a YouTube thumbnail that attracts Korean viewers based on the following video summary.
        This thumbnail must look professional, eye-catching, and optimized for Korean YouTube audiences.

        âš ï¸ LANGUAGE RULES (IMPORTANT):
        - **All visible text in the thumbnail must be written in Korean (Hangul only)**.
        - **Never use English letters or Romanized Korean.**
        - **Do not translate Korean text into English.**
        - The Korean text must be perfectly rendered â€” no broken or corrupted Hangul characters.
        - Use proper spacing, alignment, and typographic consistency for Hangul.
        - Prioritize accurate Hangul rendering over stylistic effects.
        - If the model struggles to draw Hangul text, focus on producing realistic, clean Korean lettering.
        - Use strong, readable Korean title fonts such as **Noto Sans KR**, **Pretendard Bold**, or **Nanum Gothic ExtraBold**.

        ğŸ§© INSTRUCTIONS:
        Generate a detailed visual prompt that includes:
        1. **Main visual elements** â€” Describe characters, objects, or icons that represent the topic.
        2. **Color scheme** â€” Suggest harmonious, bright color palettes suitable for Korean audiences.
        3. **Text overlay (Korean phrases only)** â€” Propose 2â€“3 short, catchy Korean phrases to display on the thumbnail.
        4. **Overall composition** â€” Describe the layout, focal points, and balance between text and visuals.

        ğŸ–‹ STYLE GUIDELINES:
        - The thumbnail should look modern, clean, and professional.
        - Avoid clutter; maintain focus on the main message.
        - Korean text should stand out clearly against the background.
        - Use contrast (dark text on bright background or vice versa).
        - Recommended aspect ratio: 16:9 (YouTube thumbnail standard).

        ğŸ’¬ EXAMPLE:
        If the summary is about "ìƒí‘œë“±ë¡ ë°©ë²•",
        - Text overlay suggestions: 
            - "5ë¶„ ë§Œì— ëë‚´ëŠ” ìƒí‘œë“±ë¡"
            - "ì´ˆë³´ë„ ê°€ëŠ¥í•œ ë¸Œëœë“œ ë“±ë¡ ê¿€íŒ"
        - Visual: smiling Korean entrepreneur holding a trademark certificate, with the KIPO (íŠ¹í—ˆì²­) building faintly visible.
        - Background: light blue and white gradient conveying trust and clarity.

        ğŸ“„ VIDEO SUMMARY:
        {summary}
    """

    response = llm.invoke(prompt)

    thumbnail_prompt = response.content

    client = OpenAI()
    image = client.images.generate(
        model="gpt-image-1",
        prompt=thumbnail_prompt,
        quality="low",
        moderation="low",
        size="auto",

    )

    image_bytes = base64.b64decode(image.data[0].b64_json)

    filename = f"thumbnail_{concept_id}.jpg"

    with open(filename, "wb") as file:
        file.write(image_bytes)

    return {
        "thumbnail_prompts": [thumbnail_prompt],
        "thumbnail_sketches": [filename]
    }


def human_feedback(state: State):
    answer = interrupt({
        "chosen_thumbnail": "ì–´ë–¤ ì¸ë„¤ì¼ì´ ê°€ì¥ ë§ˆìŒì— ë“œë‚˜ìš”?",
        "feedback": "ì¸ë„¤ì¼ì— ëŒ€í•œ í”¼ë“œë°±ì„ ì£¼ì„¸ìš”."
    })

    user_feedback = answer["user_feedback"]
    chosen_prompt = answer["chosen_prompt"]

    return {
        "user_feedback": user_feedback,
        "chosen_prompt": state["thumbnail_prompts"][chosen_prompt-1]
    }


def generate_hd_thumbnail(state: State):
    chosen_prompt = state["chosen_prompt"]
    user_feedback = state["user_feedback"]

    prompt = f"""
        You are a professional YouTube thumbnail designer. Take this original thumbnail prompt and create an enhanced version that incorporates the user's specific feedback.

        ORIGINAL PROMPT:
        {chosen_prompt}

        USER FEEDBACK TO INCORPORATE:
        {user_feedback}

        Create an enhanced prompt that:
            1. Maintains the core concept from the original prompt
            2. Specifically addresses and implements the user's feedback requests
            3. Adds professional YouTube thumbnail specifications:
                - High contrast and bold visual elements
                - Clear focal points that draw the eye
                - Professional lighting and composition
                - Optimal text placement and readability with generous padding from edges
                - Colors that pop and grab attention
                - Elements that work well at small thumbnail sizes
                - IMPORTANT: Always ensure adequate white space/padding between any text and the image borders
    """

    response = llm.invoke(prompt)

    final_thumbnail_prompt = response.content

    client = OpenAI()
    image = client.images.generate(
        model="gpt-image-1",
        prompt=final_thumbnail_prompt,
        quality="high",
        moderation="low",
        size="auto",

    )

    image_bytes = base64.b64decode(image.data[0].b64_json)

    with open("thumbnail_final.jpg", "wb") as file:
        file.write(image_bytes)


graph_builder = StateGraph(State)

graph_builder.add_node("extract_audio", extract_audio)
graph_builder.add_node("transcribe_audio", transcribe_audio)
graph_builder.add_node("summarize_chunk", summarize_chunk)
graph_builder.add_node("finalize_summary", finalize_summary)
graph_builder.add_node("generate_thumbnail", generate_thumbnail)
graph_builder.add_node("human_feedback", human_feedback)
graph_builder.add_node("generate_hd_thumbnail", generate_hd_thumbnail)


graph_builder.add_edge(START, "extract_audio")
graph_builder.add_edge("extract_audio", "transcribe_audio")
graph_builder.add_conditional_edges(
    "transcribe_audio", dispatch_summarizers, ["summarize_chunk"])
graph_builder.add_edge("summarize_chunk", "finalize_summary")
graph_builder.add_conditional_edges(
    "finalize_summary", dispatch_artists, ["generate_thumbnail"])
graph_builder.add_edge("generate_thumbnail", "human_feedback")
graph_builder.add_edge("human_feedback", "generate_hd_thumbnail")
graph_builder.add_edge("human_feedback", END)

graph = graph_builder.compile(name="mr_thumbs")
